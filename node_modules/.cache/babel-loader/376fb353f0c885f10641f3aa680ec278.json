{"ast":null,"code":"var _jsxFileName = \"/Users/bruceqin/Desktop/CS4476/project-one/src/pages/Landing/Landing.js\";\nimport React from 'react';\nimport { Typography } from '@material-ui/core';\nimport { makeStyles, useTheme } from '@material-ui/core/styles';\nimport { Header, TeamDescription } from '../../components';\nimport { Link } from 'react-router-dom';\nimport ButtonBase from '@material-ui/core/ButtonBase';\nimport mainFig from \"../../img/mainfig.png\";\nconst useStyles = makeStyles(theme => ({\n  wrapper: {\n    display: 'flex',\n    alignItems: 'center',\n    flexDirection: 'column',\n    paddingTop: '10%'\n  },\n  link: {\n    textDecoration: 'none',\n    color: '#000'\n  },\n  titleFormat: {\n    fontFamily: '-apple-system',\n    padding: '20px'\n  },\n  imageFormat: {\n    paddingBottom: '20px'\n  }\n}));\n\nconst Landing = () => {\n  const classes = useStyles();\n  const theme = useTheme();\n  return /*#__PURE__*/React.createElement(\"div\", {\n    className: classes.wrapper,\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 37,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"img\", {\n    className: classes.imageFormat,\n    src: mainFig,\n    alt: \"...\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 38,\n      columnNumber: 13\n    }\n  }), /*#__PURE__*/React.createElement(Typography, {\n    className: classes.titleFormat,\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 39,\n      columnNumber: 13\n    }\n  }, \"Detecting and recognizing text in natural scene images and optical character recognition is a challenging task computer scientists have been working on and improving for a long time. We want to work with the Street View House Number (SVHN) dataset to create a digit detector program. With the given input of an image taken from a camera or extracted from Google street view, our detector will extract the house numbers and display the predicted output based on the trained model.\"), /*#__PURE__*/React.createElement(Header, {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 43,\n      columnNumber: 13\n    }\n  }), /*#__PURE__*/React.createElement(TeamDescription, {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 44,\n      columnNumber: 13\n    }\n  }));\n};\n\nexport default Landing;","map":{"version":3,"sources":["/Users/bruceqin/Desktop/CS4476/project-one/src/pages/Landing/Landing.js"],"names":["React","Typography","makeStyles","useTheme","Header","TeamDescription","Link","ButtonBase","mainFig","useStyles","theme","wrapper","display","alignItems","flexDirection","paddingTop","link","textDecoration","color","titleFormat","fontFamily","padding","imageFormat","paddingBottom","Landing","classes"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AAEA,SAASC,UAAT,QAA2B,mBAA3B;AACA,SAASC,UAAT,EAAqBC,QAArB,QAAqC,0BAArC;AACA,SAASC,MAAT,EAAiBC,eAAjB,QAAwC,kBAAxC;AACA,SAASC,IAAT,QAAqB,kBAArB;AACA,OAAOC,UAAP,MAAuB,8BAAvB;AACA,OAAOC,OAAP,MAAoB,uBAApB;AAEA,MAAMC,SAAS,GAAGP,UAAU,CAAEQ,KAAD,KAAY;AACrCC,EAAAA,OAAO,EAAE;AACLC,IAAAA,OAAO,EAAE,MADJ;AAELC,IAAAA,UAAU,EAAE,QAFP;AAGLC,IAAAA,aAAa,EAAC,QAHT;AAILC,IAAAA,UAAU,EAAC;AAJN,GAD4B;AAOrCC,EAAAA,IAAI,EAAE;AACFC,IAAAA,cAAc,EAAE,MADd;AAEFC,IAAAA,KAAK,EAAE;AAFL,GAP+B;AAWrCC,EAAAA,WAAW,EAAE;AACTC,IAAAA,UAAU,EAAE,eADH;AAETC,IAAAA,OAAO,EAAC;AAFC,GAXwB;AAerCC,EAAAA,WAAW,EAAE;AACTC,IAAAA,aAAa,EAAC;AADL;AAfwB,CAAZ,CAAD,CAA5B;;AAoBA,MAAMC,OAAO,GAAG,MAAM;AAClB,QAAMC,OAAO,GAAGhB,SAAS,EAAzB;AACA,QAAMC,KAAK,GAAGP,QAAQ,EAAtB;AAIA,sBACI;AAAK,IAAA,SAAS,EAAEsB,OAAO,CAACd,OAAxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACI;AAAK,IAAA,SAAS,EAAEc,OAAO,CAACH,WAAxB;AAAqC,IAAA,GAAG,EAAEd,OAA1C;AAAmD,IAAA,GAAG,EAAC,KAAvD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADJ,eAEI,oBAAC,UAAD;AAAY,IAAA,SAAS,EAAEiB,OAAO,CAACN,WAA/B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,ueAFJ,eAMI,oBAAC,MAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IANJ,eAOI,oBAAC,eAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAPJ,CADJ;AAWH,CAjBD;;AAmBA,eAAeK,OAAf","sourcesContent":["import React from 'react';\n\nimport { Typography } from '@material-ui/core';\nimport { makeStyles, useTheme } from '@material-ui/core/styles';\nimport { Header, TeamDescription } from '../../components';\nimport { Link } from 'react-router-dom';\nimport ButtonBase from '@material-ui/core/ButtonBase';\nimport mainFig from \"../../img/mainfig.png\";\n\nconst useStyles = makeStyles((theme) => ({\n    wrapper: {\n        display: 'flex',\n        alignItems: 'center',\n        flexDirection:'column',\n        paddingTop:'10%',\n    },\n    link: {\n        textDecoration: 'none',\n        color: '#000',\n    },\n    titleFormat: {\n        fontFamily: '-apple-system',\n        padding:'20px',\n    },\n    imageFormat: {\n        paddingBottom:'20px',\n    }\n}));\n\nconst Landing = () => {\n    const classes = useStyles();\n    const theme = useTheme();\n\n\n\n    return (\n        <div className={classes.wrapper}>\n            <img className={classes.imageFormat} src={mainFig} alt=\"...\"></img>\n            <Typography className={classes.titleFormat}>\n                Detecting and recognizing text in natural scene images and optical character recognition is a challenging task computer scientists have been working on and improving for a long time. We want to work with the Street View House Number (SVHN) dataset to create a digit detector program.\n                With the given input of an image taken from a camera or extracted from Google street view, our detector will extract the house numbers and display the predicted output based on the trained model.\n            </Typography>\n            <Header></Header>\n            <TeamDescription></TeamDescription>\n        </div>\n    )\n}\n\nexport default Landing;"]},"metadata":{},"sourceType":"module"}